def sampler_to_hdf5(X_sampler, T, save_dir, BATCH_SIZE=64, num_imgs=45000):
    '''
    X_sampler - sampler with loaded images
    T - trained nn
    save_dir - directory where hdf5 file would be saved
    BATCH_SIZE - batch size which was provided to T
    num_imgs - expected number of images  
    '''
    loader = X_sampler.loader
    n_iters = len(loader)
    n_image=0
    h5_file = h5py.File(save_dir):
    data = h5_file.create_dataset('imgs', shape=(num_imgs, 64, 64 3), dtype=np.int64)
    freeze(T)
    for iter in range(n_iters):
        x = X_sampler.sample(BATCH_SIZE)
        T_x = T(x)
        imgs = (T_x.to('cpu').permute(0, 2, 3, 1).mul(0.5).add(0.5).numpy().clip(0, 1) * 255).astype(np.int64)
        for img in imgs:
            data[n_image] = img
            n_image+=1
            if (n_image) == num_imgs:
                break
    if n_image < num_imgs - 1:
        print(f'Warinig something wrong with batch size. Amount of pictures is less than {num_images}')
    h5_file.close()
